{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ue5hxxkdAQJg"
      },
      "source": [
        "<img src=\"https://github.com/hernancontigiani/ceia_memorias_especializacion/raw/master/Figures/logoFIUBA.jpg\" width=\"500\" align=\"center\">\n",
        "\n",
        "\n",
        "# Procesamiento de lenguaje natural\n",
        "## Preprocesamiento con NLTK y Spacy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kCED1hh-Ioyf"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import string\n",
        "import random \n",
        "\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMOa4JPSCJ29"
      },
      "source": [
        "### Datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gdW55pJin1rt"
      },
      "outputs": [],
      "source": [
        "simple_text = \"if she leaves now she might miss something important!\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RIO7b8GjAC17"
      },
      "outputs": [],
      "source": [
        "large_text = \"Patients who in late middle age have smoked 20 cigarettes a day since their teens constitute an at-risk group. One thing they’re clearly at risk for is the acute sense of guilt that a clinician can incite, which immediately makes a consultation tense.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FVHxBRNzCMOS"
      },
      "source": [
        "### 1 - Preprocesamiento con NLTK\n",
        "- Cada documento transformarlo en una lista de términos\n",
        "- Armar un vector de términos no repetidos de todos los documentos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wM-lmmsFnC6X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b6e83e2-70d5-42ff-ccf8-273a43f6e03e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize  \n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Descargar tokenizador punkt\n",
        "nltk.download(\"punkt\")\n",
        "# Descargar diccionario de inglés\n",
        "nltk.download(\"wordnet\")\n",
        "# Descargar diccionario de stopwords\n",
        "nltk.download('stopwords')\n",
        "# Para usar NLTK 3.6.6 o superior es necesario instalar OMW 1.4 \n",
        "# (Open Multilingual WordNet)\n",
        "nltk.download('omw-1.4')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "soL5T-UDsZ20"
      },
      "outputs": [],
      "source": [
        "simple_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Er9fvFonfT1"
      },
      "outputs": [],
      "source": [
        "# Crear el derivador\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.stem.porter import *\n",
        "p_stemmer = PorterStemmer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BuEob1D6nEPK"
      },
      "outputs": [],
      "source": [
        "# Crear el lematizador\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GE9pq3dMod6Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76091233-0cd0-4a62-8af1-5bfbc0a7dbff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens: ['if', 'she', 'leaves', 'now', 'she', 'might', 'miss', 'something', 'important', '!']\n"
          ]
        }
      ],
      "source": [
        "# Extraer los tokens de un doc\n",
        "tokens = word_tokenize(simple_text)\n",
        "print(\"Tokens:\", tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lSdedQvVM-wN"
      },
      "outputs": [],
      "source": [
        "# Transformar los tokens a sus respectivas palabras derivadas\n",
        "# Stemming\n",
        "nltk_stemedList = []\n",
        "for word in tokens:\n",
        "    nltk_stemedList.append(p_stemmer.stem(word))\n",
        "print(\"Stemming:\", nltk_stemedList)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OV3-wVBSNNaA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e73052f9-28b5-44fe-e4ef-19f4b0aa6a86"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lemmatization: ['if', 'she', 'leaf', 'now', 'she', 'might', 'miss', 'something', 'important', '!']\n"
          ]
        }
      ],
      "source": [
        "# Transformar los tokens a sus respectivas palabras raiz\n",
        "# Lemmatization\n",
        "nltk_lemmaList = []\n",
        "for word in tokens:\n",
        "    nltk_lemmaList.append(lemmatizer.lemmatize(word))\n",
        "print(\"Lemmatization:\", nltk_lemmaList)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jFzuIwPZuNqo"
      },
      "outputs": [],
      "source": [
        "string.punctuation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U47nxm8ZNiIr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a10b0fb4-0a05-4dc0-e467-36ab18a33b2c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Punctuation filter: ['if', 'she', 'leaf', 'now', 'she', 'might', 'miss', 'something', 'important']\n"
          ]
        }
      ],
      "source": [
        "# Quitar los signos de puntuacion\n",
        "nltk_punctuation = [w for w in nltk_lemmaList if w not in string.punctuation]\n",
        "print(\"Punctuation filter:\", nltk_punctuation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A71y7Ecsun-u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83d4df5f-ede5-4928-c1cf-d0f0a642e3f7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "179"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "nltk_stop_words = set(stopwords.words(\"english\"))\n",
        "len(nltk_stop_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ImlO-N45OuKG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "676215f1-f1a1-49a2-9397-d0d8055d1c4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stop words filter: ['leaf', 'might', 'miss', 'something', 'important']\n"
          ]
        }
      ],
      "source": [
        "# Stop words\n",
        "nltk_stop_words = set(stopwords.words(\"english\"))\n",
        "filtered_sentence = [w for w in nltk_punctuation if w not in nltk_stop_words]\n",
        "print(\"Stop words filter:\", filtered_sentence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6NrPtt2OmWBv"
      },
      "source": [
        "### 2 - Proceso completo con NLTK\n",
        "Tokenization → Lemmatization → Remove stopwords → Remove punctuation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ZqTOZzDI7uv"
      },
      "outputs": [],
      "source": [
        "def nltk_process(text):\n",
        "    # Tokenization\n",
        "    nltk_tokenList = word_tokenize(text)\n",
        "      \n",
        "    # Lemmatization\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    nltk_lemmaList = []\n",
        "    for word in nltk_tokenList:\n",
        "        nltk_lemmaList.append(lemmatizer.lemmatize(word))\n",
        "    \n",
        "    print(\"Lemmatization\")\n",
        "    print(nltk_lemmaList)\n",
        "\n",
        "    # Stop words\n",
        "    nltk_stop_words = set(stopwords.words(\"english\"))\n",
        "    filtered_sentence = [w for w in nltk_lemmaList if w not in nltk_stop_words]\n",
        "\n",
        "    # Filter Punctuation\n",
        "    filtered_sentence = [w for w in filtered_sentence if w not in string.punctuation]\n",
        "\n",
        "    print(\" \")\n",
        "    print(\"Remove stopword & Punctuation\")\n",
        "    print(filtered_sentence)\n",
        "    return filtered_sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CZdiop6IJpZN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fadf605e-0245-4793-88a1-1c2564475583"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lemmatization\n",
            "['Patients', 'who', 'in', 'late', 'middle', 'age', 'have', 'smoked', '20', 'cigarette', 'a', 'day', 'since', 'their', 'teen', 'constitute', 'an', 'at-risk', 'group', '.', 'One', 'thing', 'they', '’', 're', 'clearly', 'at', 'risk', 'for', 'is', 'the', 'acute', 'sense', 'of', 'guilt', 'that', 'a', 'clinician', 'can', 'incite', ',', 'which', 'immediately', 'make', 'a', 'consultation', 'tense', '.']\n",
            " \n",
            "Remove stopword & Punctuation\n",
            "['Patients', 'late', 'middle', 'age', 'smoked', '20', 'cigarette', 'day', 'since', 'teen', 'constitute', 'at-risk', 'group', 'One', 'thing', '’', 'clearly', 'risk', 'acute', 'sense', 'guilt', 'clinician', 'incite', 'immediately', 'make', 'consultation', 'tense']\n",
            "Text len: 27\n"
          ]
        }
      ],
      "source": [
        "nltk_text = nltk_process(large_text)\n",
        "print(\"Text len:\", len(nltk_text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_M4F0ll1msUY"
      },
      "source": [
        "### 3 - Proceso completo con spaCy\n",
        "Tokenization → Lemmatization → Remove stopwords → Remove punctuation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r57e9b9Omwnh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9fbe36ed-fd90-4b46-a3a8-45758733f4ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
            "  warnings.warn(\"Can't initialize NVML\")\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "# Cargar pipeline de preprocesamiento de inglés\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "def spacy_process(text):\n",
        "    doc = nlp(text)\n",
        "    \n",
        "    # Tokenization & lemmatization\n",
        "    lemma_list = []\n",
        "    for token in doc:\n",
        "        lemma_list.append(token.lemma_)\n",
        "    print(\"Tokenize+Lemmatize:\")\n",
        "    print(lemma_list)\n",
        "    \n",
        "    # Stop words\n",
        "    filtered_sentence =[]\n",
        "    for word in lemma_list:\n",
        "        # word es un string, para recuperar la información de los objetos de SpaCy\n",
        "        # necesitamos usar el string para pasar a un lexema, el objeto de SpaCy\n",
        "        # que para cada término contiene la información del preprocesamiento\n",
        "        # (se podría también directamente filtrar stopwords en el paso de lematización)\n",
        "        lexeme = nlp.vocab[word]\n",
        "        if lexeme.is_stop == False:\n",
        "            filtered_sentence.append(word) \n",
        "    \n",
        "    # Filter punctuation\n",
        "    filtered_sentence = [w for w in filtered_sentence if w not in string.punctuation]\n",
        "\n",
        "    print(\" \")\n",
        "    print(\"Remove stopword & punctuation: \")\n",
        "    print(filtered_sentence)\n",
        "    return filtered_sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9x_iKHu1pKBE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "529696c1-c572-4786-cccf-76d2fa535254"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenize+Lemmatize:\n",
            "['patient', 'who', 'in', 'late', 'middle', 'age', 'have', 'smoke', '20', 'cigarette', 'a', 'day', 'since', 'their', 'teen', 'constitute', 'an', 'at', '-', 'risk', 'group', '.', 'one', 'thing', 'they', '’re', 'clearly', 'at', 'risk', 'for', 'be', 'the', 'acute', 'sense', 'of', 'guilt', 'that', 'a', 'clinician', 'can', 'incite', ',', 'which', 'immediately', 'make', 'a', 'consultation', 'tense', '.']\n",
            " \n",
            "Remove stopword & punctuation: \n",
            "['patient', 'late', 'middle', 'age', 'smoke', '20', 'cigarette', 'day', 'teen', 'constitute', 'risk', 'group', 'thing', 'clearly', 'risk', 'acute', 'sense', 'guilt', 'clinician', 'incite', 'immediately', 'consultation', 'tense']\n",
            "Text len: 27\n"
          ]
        }
      ],
      "source": [
        "spacy_text = spacy_process(large_text)\n",
        "print(\"Text len:\", len(nltk_text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txMZ7lR-pvHB"
      },
      "source": [
        "### 4 - Conclusiones\n",
        "- NLTK no pasa a minúsculas el texto por su cuenta\n",
        "- spacy algunas palabras las reemplaza por su Tag (como \"'\")\n",
        "- spacy descompone palabras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Te3GgSNzpbGq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1abd7c86-1b8b-4f9c-a459-e81207ebc6b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+--------------+\n",
            "|    NLTK    |    spaCy     |\n",
            "+------------+--------------+\n",
            "|  Patients  |   patient    |\n",
            "|    late    |     late     |\n",
            "|   middle   |    middle    |\n",
            "|    age     |     age      |\n",
            "|   smoked   |    smoke     |\n",
            "|     20     |      20      |\n",
            "| cigarette  |  cigarette   |\n",
            "|    day     |     day      |\n",
            "|   since    |     teen     |\n",
            "|    teen    |  constitute  |\n",
            "| constitute |     risk     |\n",
            "|  at-risk   |    group     |\n",
            "|   group    |    thing     |\n",
            "|    One     |   clearly    |\n",
            "|   thing    |     risk     |\n",
            "|     ’      |    acute     |\n",
            "|  clearly   |    sense     |\n",
            "|    risk    |    guilt     |\n",
            "|   acute    |  clinician   |\n",
            "|   sense    |    incite    |\n",
            "|   guilt    | immediately  |\n",
            "| clinician  | consultation |\n",
            "|   incite   |    tense     |\n",
            "+------------+--------------+\n"
          ]
        }
      ],
      "source": [
        "from prettytable import PrettyTable\n",
        "table = PrettyTable(['NLTK', 'spaCy'])\n",
        "for nltk_word, spacy_word in zip(nltk_text, spacy_text):\n",
        "    table.add_row([nltk_word, spacy_word])\n",
        "print(table)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}